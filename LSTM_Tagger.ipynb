{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gu4PEgl8zpL",
        "outputId": "64b10e4a-ba51-4a36-8beb-a25984721548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xGIy4Yb082b8"
      },
      "outputs": [],
      "source": [
        "from conllu import parse,TokenList,Token\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import build_vocab_from_iterator,Vocab\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QYrySmta846y"
      },
      "outputs": [],
      "source": [
        "UNKNOWN_TOKEN = \"<unk>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "EMBEDDING_SIZE=256\n",
        "HIDDEN_DIM=256\n",
        "NUM_STACKS=2\n",
        "BATCH_SIZE=256\n",
        "lrate=0.001\n",
        "EPOCHS=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m8KBPbwi87za"
      },
      "outputs": [],
      "source": [
        "def filter_sentences_by_tag(sentences, pos_tags, tag_to_exclude='SYM'): # removing SYM tag sentences\n",
        "    filtered_sentences = []\n",
        "    filtered_pos_tags = []\n",
        "    for sentence, tags in zip(sentences, pos_tags):\n",
        "        if tag_to_exclude not in tags:\n",
        "            filtered_sentences.append(sentence)\n",
        "            filtered_pos_tags.append(tags)\n",
        "    return filtered_sentences, filtered_pos_tags\n",
        "def extract_tokens_and_tags(sentences):\n",
        "    token_sequences = []\n",
        "    tag_sequences = []\n",
        "    for sentence in sentences:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        for token in sentence:\n",
        "            tokens.append(token[\"form\"])\n",
        "            tags.append(token[\"upos\"])\n",
        "        token_sequences.append(tokens)\n",
        "        tag_sequences.append(tags)\n",
        "    return token_sequences, tag_sequences\n",
        "def replace_low_frequency_words(sentences, threshold=3):\n",
        "    word_counts = Counter(word for sentence in sentences for word in sentence)\n",
        "    replaced_sentences = [\n",
        "        [UNKNOWN_TOKEN if word_counts[word] < threshold else word for word in sentence]\n",
        "        for sentence in sentences\n",
        "    ]\n",
        "    return replaced_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qNlK0Btw8-AW"
      },
      "outputs": [],
      "source": [
        "class EntityDataset_LSTM(Dataset):\n",
        "  def __init__(self, sent, labs, vocabulary:Vocab|None=None):\n",
        "    \"\"\"Initialize the dataset. Setup Code goes here\"\"\"\n",
        "    self.sentences = sent\n",
        "    self.labels = labs\n",
        "    if vocabulary is None:\n",
        "      self.vocabulary = build_vocab_from_iterator(self.sentences, specials=[UNKNOWN_TOKEN, PAD_TOKEN])\n",
        "      self.vocabulary.set_default_index(self.vocabulary[UNKNOWN_TOKEN])\n",
        "    else:\n",
        "      self.vocabulary = vocabulary\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns number of datapoints.\"\"\"\n",
        "    return len(self.sentences)\n",
        "\n",
        "  def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get the datapoint at `index`.\"\"\"\n",
        "    return torch.tensor(self.vocabulary.lookup_indices(self.sentences[index])), torch.tensor(self.labels[index])\n",
        "\n",
        "  def collate(self, batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Given a list of datapoints, batch them together\"\"\"\n",
        "    sentences = [i[0] for i in batch]\n",
        "    labels = [i[1] for i in batch]\n",
        "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=self.vocabulary[PAD_TOKEN])\n",
        "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=torch.tensor(0))\n",
        "    return padded_sentences, padded_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EbgNFIpY9FKp"
      },
      "outputs": [],
      "source": [
        "def get_datasets_LSTM():\n",
        "  with open('/content/en_atis-ud-train.conllu') as f:\n",
        "    train_sentences = parse(f.read())\n",
        "  tok_seq,tag_seq=extract_tokens_and_tags(train_sentences)\n",
        "  tok_seq=replace_low_frequency_words(tok_seq)\n",
        "  unique_tags = set(tag for tags in tag_seq for tag in tags)\n",
        "  tag_to_id = {tag: idx+1 for idx, tag in enumerate(sorted(unique_tags))}\n",
        "  print(tag_to_id)\n",
        "  new_tag_seq=[[tag_to_id[tag] for tag in tags] for tags in tag_seq]\n",
        "  train_dataset=EntityDataset_LSTM(tok_seq,new_tag_seq)\n",
        "  with open('/content/en_atis-ud-test.conllu') as f:\n",
        "    test_sentences = parse(f.read())\n",
        "  with open('/content/en_atis-ud-dev.conllu') as f:\n",
        "    val_sentences = parse(f.read())\n",
        "  test_toks,test_tags=extract_tokens_and_tags(test_sentences)\n",
        "  val_toks,val_tags=extract_tokens_and_tags(val_sentences)\n",
        "  val_toks,val_tags=filter_sentences_by_tag(val_toks,val_tags)\n",
        "  test_tags=[[tag_to_id[tag] for tag in tags] for tags in test_tags]\n",
        "  val_tags=[[tag_to_id[tag] for tag in tags] for tags in val_tags]\n",
        "  val_dataset=EntityDataset_LSTM(val_toks,val_tags,vocabulary=train_dataset.vocabulary)\n",
        "  test_dataset=EntityDataset_LSTM(test_toks,test_tags,vocabulary=train_dataset.vocabulary)\n",
        "  return train_dataset,val_dataset,test_dataset,tag_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRLYnboK9IBs",
        "outputId": "a82080cd-e01c-41a6-9165-cf07d12983e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ADJ': 1, 'ADP': 2, 'ADV': 3, 'AUX': 4, 'CCONJ': 5, 'DET': 6, 'INTJ': 7, 'NOUN': 8, 'NUM': 9, 'PART': 10, 'PRON': 11, 'PROPN': 12, 'VERB': 13}\n"
          ]
        }
      ],
      "source": [
        "train_dataset,val_dataset,test_dataset,tag_to_id=get_datasets_LSTM()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "l8pn9VYs9J1U"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=train_dataset.collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,collate_fn=val_dataset.collate)\n",
        "test_dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE,collate_fn=test_dataset.collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "azxlO_j69Lg3"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(torch.nn.Module):\n",
        "  def __init__(self, embedding_dim: int, hidden_dim: int, vocabulary_size: int, tagset_size: int, stacks: int):\n",
        "    super().__init__()\n",
        "    self.embedding_module = torch.nn.Embedding(vocabulary_size, embedding_dim)\n",
        "    self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, stacks)\n",
        "    self.hidden_to_tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
        "  def forward(self, sentence: torch.Tensor):\n",
        "    embeddings = self.embedding_module(sentence)\n",
        "    lstm_out, _ = self.lstm(embeddings)\n",
        "    return self.hidden_to_tag(lstm_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "buqEeDvg9NVy",
        "outputId": "74c3047a-f942-4ce6-84b0-d8ac99a55bee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(\"mps\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6cPL5QFv9QkC"
      },
      "outputs": [],
      "source": [
        "entity_predictor=LSTMModel(EMBEDDING_SIZE,HIDDEN_DIM,len(train_dataset.vocabulary),len(tag_to_id)+1,NUM_STACKS)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(entity_predictor.parameters(),lr=lrate)\n",
        "entity_predictor = entity_predictor.to(device)\n",
        "dev_set_acc=[]\n",
        "for epoch_num in range(EPOCHS):\n",
        "  entity_predictor.train()\n",
        "  for batch_num, (words, tags) in enumerate(train_dataloader):\n",
        "    (words, tags) = (words.to(device), tags.to(device))\n",
        "    one_hot_tags=(torch.nn.functional.one_hot(tags, num_classes=len(tag_to_id)+1)).float()\n",
        "    pred = entity_predictor(words)\n",
        "    loss = loss_fn(pred, one_hot_tags)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  # entity_predictor.eval()\n",
        "  # with torch.no_grad():\n",
        "  #   test_loss = 0\n",
        "  #   for batch_num, (words, tags) in enumerate(val_dataloader):\n",
        "  #     (words, tags) = (words.to(device), tags.to(device))\n",
        "  #     one_hot_tags=(torch.nn.functional.one_hot(tags, num_classes=len(tag_to_id)+1)).float()\n",
        "  #     pred = entity_predictor(words)\n",
        "  #     test_loss += (loss_fn(pred, one_hot_tags)).item()\n",
        "  # print(f\"Validation error: {test_loss/len(val_dataloader)}\")\n",
        "  entity_predictor.eval()\n",
        "  predictions=[]\n",
        "  true_vals=[]\n",
        "  with torch.no_grad():\n",
        "    for batch_num, (words, tags) in enumerate(val_dataloader):\n",
        "      (words, tags) = (words.to(device), tags.to(device))\n",
        "      pred = entity_predictor(words)\n",
        "      pred_max_index = torch.argmax(pred, dim=2)\n",
        "      true_vals.extend(tags.flatten().cpu())\n",
        "      predictions.extend(pred_max_index.flatten().cpu())\n",
        "  predictions=torch.stack(predictions).numpy()\n",
        "  true_vals=torch.stack(true_vals).numpy()\n",
        "  dev_set_acc.append(accuracy_score(true_vals,predictions))\n",
        "plt.bar(range(1,EPOCHS+1), dev_set_acc)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy on dev set')\n",
        "plt.title('epoch number vs dev set accuracy for LSTM POS Tagger')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gq2dWC6f_tpt"
      },
      "outputs": [],
      "source": [
        "entity_predictor.eval()\n",
        "predictions=[]\n",
        "true_vals=[]\n",
        "with torch.no_grad():\n",
        "  for batch_num, (words, tags) in enumerate(val_dataloader):\n",
        "    (words, tags) = (words.to(device), tags.to(device))\n",
        "    pred = entity_predictor(words)\n",
        "    pred_max_index = torch.argmax(pred, dim=2)\n",
        "    true_vals.extend(tags.flatten().cpu())\n",
        "    predictions.extend(pred_max_index.flatten().cpu())\n",
        "predictions=torch.stack(predictions).numpy()\n",
        "true_vals=torch.stack(true_vals).numpy()\n",
        "f1_micro=f1_score(true_vals,predictions,average='micro')\n",
        "f1_macro=f1_score(true_vals,predictions,average='macro')\n",
        "rec_micro=recall_score(true_vals,predictions,average='micro')\n",
        "rec_macro=recall_score(true_vals,predictions,average='macro')\n",
        "pre_micro=precision_score(true_vals,predictions,average='micro')\n",
        "pre_macro=precision_score(true_vals,predictions,average='macro')\n",
        "print(f'Accuracy Score on dev set: {accuracy_score(true_vals,predictions)}')\n",
        "print(f'F1-Score(micro) on dev set: {f1_micro}')\n",
        "print(f'F1-Score(macro) on dev set: {f1_macro}')\n",
        "print(f'Recall(micro) Score on dev set: {rec_micro}')\n",
        "print(f'Recall(macro) Score on dev set: {rec_macro}')\n",
        "print(f'Precision(micro) Score on dev set: {pre_micro}')\n",
        "print(f'Precision(macro) Score on dev set: {pre_macro}')\n",
        "print(f'Confusion matrix for dev set:\\n {confusion_matrix(true_vals,predictions)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q6_DbrQu9ShT"
      },
      "outputs": [],
      "source": [
        "entity_predictor.eval()\n",
        "predictions=[]\n",
        "true_vals=[]\n",
        "with torch.no_grad():\n",
        "  for batch_num, (words, tags) in enumerate(test_dataloader):\n",
        "    (words, tags) = (words.to(device), tags.to(device))\n",
        "    pred = entity_predictor(words)\n",
        "    pred_max_index = torch.argmax(pred, dim=2)\n",
        "    true_vals.extend(tags.flatten().cpu())\n",
        "    predictions.extend(pred_max_index.flatten().cpu())\n",
        "predictions=torch.stack(predictions).numpy()\n",
        "true_vals=torch.stack(true_vals).numpy()\n",
        "f1_micro=f1_score(true_vals,predictions,average='micro')\n",
        "f1_macro=f1_score(true_vals,predictions,average='macro')\n",
        "rec_micro=recall_score(true_vals,predictions,average='micro')\n",
        "rec_macro=recall_score(true_vals,predictions,average='macro')\n",
        "pre_micro=precision_score(true_vals,predictions,average='micro')\n",
        "pre_macro=precision_score(true_vals,predictions,average='macro')\n",
        "print(f'Accuracy Score on test set: {accuracy_score(true_vals,predictions)}')\n",
        "print(f'F1-Score(micro) on test set: {f1_micro}')\n",
        "print(f'F1-Score(macro) on test set: {f1_macro}')\n",
        "print(f'Recall(micro) Score on test set: {rec_micro}')\n",
        "print(f'Recall(macro) Score on test set: {rec_macro}')\n",
        "print(f'Precision(micro) Score on test set: {pre_micro}')\n",
        "print(f'Precision(macro) Score on test set: {pre_macro}')\n",
        "print(f'Confusion matrix for test set:\\n {confusion_matrix(true_vals,predictions)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bliET-WtHbA3"
      },
      "outputs": [],
      "source": [
        "# for BATCH_SIZE in [8,16,32,64,128]:\n",
        "#   for EMBEDDING_SIZE in [32,64,128,256]:\n",
        "#     for HIDDEN_DIM in [128,256,512]:\n",
        "#       train_dataset,val_dataset,test_dataset,tag_to_id=get_datasets_LSTM()\n",
        "#       train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=train_dataset.collate)\n",
        "#       val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,collate_fn=val_dataset.collate)\n",
        "#       test_dataloader=DataLoader(test_dataset, batch_size=BATCH_SIZE,collate_fn=test_dataset.collate)\n",
        "#       entity_predictor=LSTMModel(EMBEDDING_SIZE,HIDDEN_DIM,len(train_dataset.vocabulary),len(tag_to_id)+1,NUM_STACKS)\n",
        "#       loss_fn = torch.nn.CrossEntropyLoss()\n",
        "#       optimizer = torch.optim.Adam(entity_predictor.parameters(),lr=lrate)\n",
        "#       entity_predictor = entity_predictor.to(device)\n",
        "\n",
        "#       for epoch_num in range(EPOCHS):\n",
        "#         entity_predictor.train()\n",
        "#         for batch_num, (words, tags) in enumerate(train_dataloader):\n",
        "#           (words, tags) = (words.to(device), tags.to(device))\n",
        "#           one_hot_tags=(torch.nn.functional.one_hot(tags, num_classes=len(tag_to_id)+1)).float()\n",
        "#           pred = entity_predictor(words)\n",
        "#           loss = loss_fn(pred, one_hot_tags)\n",
        "#           optimizer.zero_grad()\n",
        "#           loss.backward()\n",
        "#           optimizer.step()\n",
        "#       entity_predictor.eval()\n",
        "#       predictions=[]\n",
        "#       true_vals=[]\n",
        "#       with torch.no_grad():\n",
        "#         for batch_num, (words, tags) in enumerate(test_dataloader):\n",
        "#           (words, tags) = (words.to(device), tags.to(device))\n",
        "#           pred = entity_predictor(words)\n",
        "#           pred_max_index = torch.argmax(pred, dim=2)\n",
        "#           true_vals.extend(tags.flatten().cpu())\n",
        "#           predictions.extend(pred_max_index.flatten().cpu())\n",
        "#       predictions=torch.stack(predictions).numpy()\n",
        "#       true_vals=torch.stack(true_vals).numpy()\n",
        "#       non_zero_indices = np.where(true_vals != 0)\n",
        "#       true_vals = true_vals[non_zero_indices]\n",
        "#       predictions = predictions[non_zero_indices]\n",
        "#       # f1_micro=f1_score(true_vals,predictions,average='micro')\n",
        "#       # f1_macro=f1_score(true_vals,predictions,average='macro')\n",
        "#       # rec_micro=recall_score(true_vals,predictions,average='micro')\n",
        "#       # rec_macro=recall_score(true_vals,predictions,average='macro')\n",
        "#       # pre_micro=precision_score(true_vals,predictions,average='micro')\n",
        "#       # pre_macro=precision_score(true_vals,predictions,average='macro')\n",
        "#       print(f'Accuracy Score on test set: {BATCH_SIZE, EMBEDDING_SIZE, HIDDEN_DIM, accuracy_score(true_vals,predictions)}')\n",
        "#       # print(f'F1-Score(micro) on test set: {f1_micro}')\n",
        "#       # print(f'F1-Score(macro) on test set: {f1_macro}')\n",
        "#       # print(f'Recall(micro) Score on test set: {rec_micro}')\n",
        "#       # print(f'Recall(macro) Score on test set: {rec_macro}')\n",
        "#       # print(f'Precision(micro) Score on test set: {pre_micro}')\n",
        "#       # print(f'Precision(macro) Score on test set: {pre_macro}')\n",
        "#       # print(f'Confusion matrix for test set:\\n {confusion_matrix(true_vals,predictions)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Fi8iW8Y_JTPM"
      },
      "outputs": [],
      "source": [
        "entity_predictor=torch.load('LSTM_POS_Tagger.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "le_oiCsiJtXl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G_ZCkYfJzai",
        "outputId": "cde921a8-01a2-4c45-a449-d6670db18777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'ADJ', 2: 'ADP', 3: 'ADV', 4: 'AUX', 5: 'CCONJ', 6: 'DET', 7: 'INTJ', 8: 'NOUN', 9: 'NUM', 10: 'PART', 11: 'PRON', 12: 'PROPN', 13: 'VERB'}\n"
          ]
        }
      ],
      "source": [
        "def tokenise(text):\n",
        "    tokenizer=RegexpTokenizer(r'\\w+')\n",
        "    tokens=tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "def reverse_dict(original_dict):\n",
        "    reverse_dict = {v: k for k, v in original_dict.items()}\n",
        "    return reverse_dict\n",
        "rev_tag_to_id=reverse_dict(tag_to_id)\n",
        "print(rev_tag_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQfS4ogeJ25f",
        "outputId": "50adda88-2725-429b-ac4e-de8d9ea1a352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi, they go .\n",
            "['hi', 'they', 'go']\n"
          ]
        }
      ],
      "source": [
        "user_inp=input()\n",
        "tok_sent=tokenise(user_inp)\n",
        "orig_sent=[]\n",
        "for x in tok_sent:\n",
        "  orig_sent.append(x)\n",
        "print(tok_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8fRpES6J-U-",
        "outputId": "3b013698-76b5-4202-f999-2f9d8b4a78ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[11,  4,  6, 12,  8,  2, 12]])\n"
          ]
        }
      ],
      "source": [
        "ntok_sent=tok_sent\n",
        "with torch.no_grad():\n",
        "  temp=[train_dataset.vocabulary.lookup_indices(ntok_sent)]\n",
        "  temp=torch.tensor(temp)\n",
        "  res=entity_predictor(temp)\n",
        "  pred_max_index = torch.argmax(res, dim=2)\n",
        "  max_values = torch.max(res, dim=2).values\n",
        "  mask = (pred_max_index == 0)\n",
        "  next_max_indices = torch.argsort(res, dim=2)[:, :, -2]\n",
        "  pred_max_index = torch.where(mask, next_max_indices, pred_max_index)\n",
        "  print(pred_max_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMspKl9IKDst",
        "outputId": "c4c366f0-0c6f-4d7f-cc27-ddc2255b0b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what PRON\n",
            "are AUX\n",
            "the DET\n",
            "coach PROPN\n",
            "flights NOUN\n",
            "between ADP\n",
            "dallas PROPN\n"
          ]
        }
      ],
      "source": [
        "for a,t in zip(orig_sent,pred_max_index[0]):\n",
        "    print(a+\" \"+rev_tag_to_id[int(t)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
